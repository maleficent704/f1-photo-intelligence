# F1 Photo Intelligence - Session Handoff

**Date:** 2026-02-11
**Session Duration:** ~3 hours
**Status:** Layer 1 (CLIP) Complete ‚úÖ | Layer 2 (VLM) In Progress ‚è≥

---

## What's Been Completed ‚úÖ

### 1. CLIP Embeddings Layer (100% Complete)
**Status:** Fully working and tested

**Achievements:**
- Processed all 9,521 photos with CLIP embeddings
- Generated vector embeddings stored in ChromaDB
- Implemented filename-based GP metadata extraction
  - Format: `YYYY GP_Name GP - Description.jpg`
  - Example: `2021 Abu Dhabi GP - Carlos Sainz.jpg` ‚Üí `abu-dhabi-2021`
- Natural language search fully functional

**Testing:**
```bash
venv/Scripts/python.exe demo_search.py
```

**Search Examples That Work:**
- `search.search(query='red car on track', n_results=50)` ‚Üí Finds ~1000+ Ferrari photos
- `search.at_race('abu-dhabi-2021')` ‚Üí All Abu Dhabi 2021 photos
- Works on all 9,521 photos

**Files:**
- ‚úÖ `src/indexing/clip_embedder.py` - Working
- ‚úÖ `src/query/photo_search.py` - Working
- ‚úÖ `demo_search.py` - Working demo script
- ‚úÖ `data/f1_photos.db` - 9,521 photos registered
- ‚úÖ `data/chroma_db/` - All embeddings stored

---

### 2. VLM Prompt Engineering (Complete)
**Status:** Prompt created and tested successfully

**Achievements:**
- Created improved F1-specific prompt with:
  - Team identification guide (Ferrari=red, Aston Martin=green, etc.)
  - Track identification hints
  - Session type detection
  - Detailed F1-specific instructions
- Tested on sample photos with excellent results

**Test Results:**
```
Photo: 2021 Abu Dhabi GP - Aston Martin Team Photo.jpg
‚úÖ Team: Aston Martin (correctly identified by green cars)
‚úÖ Track: Abu Dhabi
‚úÖ Session: team_photo
‚úÖ Car numbers: 1, 2
‚úÖ People count: 15

Photo: 2021 Abu Dhabi GP - Carlos Sainz.jpg
‚úÖ Team: Ferrari (correctly identified by red car)
‚úÖ Car number: 56
‚úÖ Description: "predominantly red with white and black accents"
```

**The improved prompt works!** It successfully identifies teams by car color, which the original prompt failed to do.

**Files:**
- ‚úÖ `src/indexing/vlm_describer.py` - Updated with improved prompt
- ‚úÖ `improved_prompt.txt` - Reference copy of improved prompt

---

### 3. Project Setup & Documentation
**Status:** Complete

**Created:**
- ‚úÖ `SETUP_GUIDE.md` - Comprehensive reference (all 3 layers explained)
- ‚úÖ `requirements-no-faces.txt` - Dependencies without face recognition
- ‚úÖ `demo_search.py` - Interactive search demo
- ‚úÖ `test_filename_parser.py` - Tests GP extraction from filenames
- ‚úÖ Project structure with proper Python packages

**Installed:**
- ‚úÖ Python 3.12 virtual environment
- ‚úÖ PyTorch 2.10.0 (CPU version - GPU install failed due to network)
- ‚úÖ sentence-transformers (CLIP)
- ‚úÖ ChromaDB
- ‚úÖ Ollama 0.15.6
- ‚úÖ LLaVA 7B vision model

**Not Installed:**
- ‚ùå face_recognition (requires CMake - Windows issue)
- ‚ùå dlib (requires CMake)
- ‚ùå GPU PyTorch (network connectivity issues)

---

## What's In Progress ‚è≥

### VLM Full Processing (Layer 2)
**Status:** Technical issue blocking full batch processing

**What Works:**
- ‚úÖ Manual testing: `describer.describe_photo(path)` works perfectly
- ‚úÖ Single photo batch: `describer.process_batch(limit=1)` works
- ‚úÖ Ollama is running: `ollama list` shows llava:7b installed
- ‚úÖ API endpoint works: `curl http://localhost:11434/api/generate` succeeds

**What Doesn't Work:**
- ‚ùå Full pipeline: Gets 404 errors from Ollama when processing multiple photos
- Error: `404 Client Error: Not Found for url: http://localhost:11434/api/generate`

**The Mystery:**
- Same endpoint works with curl
- Same code works when testing 1 photo manually
- Fails when pipeline runs on multiple photos
- Inconsistent - sometimes works, sometimes doesn't

**Theories:**
1. Ollama rate limiting or connection pool issues
2. Request timeout/connection reuse problem
3. Module import/reload issue in pipeline vs manual testing
4. Race condition with Ollama server

---

## Next Steps to Continue üéØ

### Option 1: Debug VLM Pipeline Issue (Recommended First)

**Try these in order:**

1. **Check if it's a simple module reload issue:**
   ```bash
   # Restart Python completely and try again
   venv/Scripts/python.exe src/pipeline/process_photos.py \
     "C:\Users\kelly\OneDrive\Pictures\f1 photos\Formula 1-3-001\Formula 1" \
     --skip-clip --skip-faces --vlm-limit 10
   ```

2. **Add request delays to avoid overwhelming Ollama:**
   - Edit `src/indexing/vlm_describer.py`
   - Add `import time` and `time.sleep(0.5)` after each request
   - This might be a rate limiting issue

3. **Check Ollama logs:**
   ```bash
   # On Windows, check if there's an Ollama log file
   # Usually in: C:\Users\kelly\.ollama\logs or similar
   ```

4. **Try restarting Ollama:**
   ```bash
   # Close Ollama completely
   # Restart: ollama serve
   # Then retry pipeline
   ```

5. **Test with smaller batches:**
   ```bash
   # Process 100 photos at a time instead of all 9,521
   venv/Scripts/python.exe -c "
   from src.indexing.vlm_describer import VLMDescriber
   describer = VLMDescriber(model='llava:7b')
   result = describer.process_batch(limit=100)
   print(f'Processed: {result}')
   "
   ```

### Option 2: Alternative Approach

**If debugging is too painful, try:**

1. **Use a different vision model:**
   ```bash
   ollama pull llava:13b  # Larger, might be more stable
   # Or try a completely different model
   ollama pull minicpm-v:8b
   ```

2. **Process in smaller batches manually:**
   ```python
   # Run this script, let it process 500 at a time
   from src.indexing.vlm_describer import VLMDescriber
   import time

   describer = VLMDescriber(model='llava:7b')

   for i in range(20):  # 20 batches of 500 = 10,000 photos
       print(f"Batch {i+1}/20")
       result = describer.process_batch(limit=500)
       print(f"Processed {result} photos")
       time.sleep(30)  # Rest between batches
   ```

3. **Skip VLM for now and add face recognition instead:**
   - Install CMake: https://cmake.org/download/
   - Run face recognition layer
   - Come back to VLM later

---

## Current Database Status

Check anytime with:
```python
import sqlite3
conn = sqlite3.connect('./data/f1_photos.db')
print(f"Total: {conn.execute('SELECT COUNT(*) FROM photos').fetchone()[0]}")
print(f"CLIP: {conn.execute('SELECT COUNT(*) FROM photos WHERE clip_embedded_at IS NOT NULL').fetchone()[0]}")
print(f"VLM: {conn.execute('SELECT COUNT(*) FROM photos WHERE vlm_described_at IS NOT NULL').fetchone()[0]}")
print(f"Faces: {conn.execute('SELECT COUNT(*) FROM photos WHERE faces_scanned_at IS NOT NULL').fetchone()[0]}")
conn.close()
```

**Current State:**
- Total photos: 9,521
- CLIP embedded: 9,521 ‚úÖ
- VLM described: 1 (test photo)
- Faces scanned: 0

---

## Important Files Modified This Session

**New Files:**
- `SETUP_GUIDE.md` - Complete reference documentation
- `HANDOFF.md` - This file
- `demo_search.py` - Interactive search demo
- `improved_prompt.txt` - Reference copy of VLM prompt
- `requirements-no-faces.txt` - Dependencies without CMake
- `test_filename_parser.py` - Tests filename parsing
- `data/face_references/README.md` - Face recognition setup guide

**Modified Files:**
- `src/indexing/clip_embedder.py` - Added filename-based GP extraction
- `src/indexing/vlm_describer.py` - Added improved F1-specific prompt + error handling
- `src/pipeline/process_photos.py` - Made face_scanner import conditional

**Files to Commit:**
All of the above

---

## Known Issues

### 1. GPU PyTorch Installation Failed
**Issue:** Network connectivity problems prevented GPU PyTorch install
**Impact:** CLIP runs on CPU (~0.4s per photo instead of ~1s with GPU)
**Status:** Not critical - CPU works fine
**Fix:** Can retry later with better network

### 2. Face Recognition Not Available
**Issue:** dlib requires CMake which isn't installed on Windows
**Impact:** Can't identify people in photos yet
**Status:** Optional feature
**Fix:** Install CMake or skip this layer

### 3. VLM Pipeline 404 Errors
**Issue:** Ollama API returns 404 when processing multiple photos
**Impact:** Can't complete full VLM processing
**Status:** Actively debugging
**Fix:** See "Next Steps" above

---

## What The User Can Do Right Now

### Search 9,521 Photos:
```bash
venv/Scripts/python.exe demo_search.py
```

### Check What's Searchable:
```python
from src.query.photo_search import F1PhotoSearch
search = F1PhotoSearch()

# Find all Ferrari photos
results = search.search(query='red car', n_results=100)

# Search specific race
results = search.at_race('monaco-2024')

# Natural language
results = search.search(query='wet race conditions')
results = search.search(query='podium celebration')
results = search.search(query='pit stop')
```

### Read Documentation:
- `README.md` - Project overview
- `SETUP_GUIDE.md` - Complete reference
- `f1-photo-intelligence.md` - Original design doc

---

## Performance Stats

**Photo Collection:**
- Total photos: 9,521
- Location: `C:\Users\kelly\OneDrive\Pictures\f1 photos\Formula 1-3-001\Formula 1\`
- Format: `YYYY GP_Name GP - Description.jpg`

**Processing Times (Measured):**
- CLIP: 63 minutes total (~0.4s per photo on CPU) ‚úÖ
- VLM: Estimated 13-26 hours (~5-10s per photo) ‚è≥
- Face: Not tested yet

**Storage:**
- Database: ~50 MB
- CLIP embeddings: ~200 MB
- Total: ~250 MB (photos stay in original location)

---

## Git Status

**Current Branch:** main

**Commits To Make:**
1. Add improved VLM prompt and error handling
2. Add comprehensive documentation (SETUP_GUIDE.md, HANDOFF.md)
3. Add demo scripts and utilities

**Not Committed Yet:**
- `data/` directory (in .gitignore - correct)
- `logs/` directory (in .gitignore - correct)
- Virtual environment (in .gitignore - correct)

---

## Questions for Next Session

1. **VLM 404 Error:** Is this an Ollama version issue? API change?
2. **Rate Limiting:** Does Ollama have undocumented rate limits?
3. **Alternative Models:** Should we try qwen2-vl if we can find it, or stick with llava?
4. **Batch Size:** What's the optimal batch size to avoid Ollama issues?

---

## Success Criteria for "Done"

- [x] CLIP layer complete and working
- [ ] VLM layer processing all 9,521 photos
- [ ] Face recognition layer (optional)
- [ ] All three layers searchable together
- [x] Documentation complete
- [ ] Code committed to GitHub

**Current Progress:** 60% complete (1.5 of 3 layers done)

---

*Last updated: 2026-02-11 19:45*
*Next session: Debug VLM Ollama 404 issue*
*Contact: Continue in same project directory*
